Data Analysis / Science Interview Questions
================

### Statistics and Probability

#### Basic Statistics

1.  Measures of **Center** (Mean, Median, Mode)
    
      - Mean: the average of our data points
      - Median: the middle point of our **sorted** data points

<!-- end list -->

``` r
# Create the vector.
x <- c(12,7,3,4.2,18,2,54,-21,8,-5)
# Find the median.
median.result <- median(x)
print(median.result)
```

    ## [1] 5.6

  - Mode: the most frequent number in our data points

If the data is normally distributed, then the values of **mean, median,
and mode are all identical**. \* If the data distribution is
**left-skewed**, then the mean \< median. \* If **right-skewed**, then
the mean \> median.

2.  Measures of **Spread** (Standard Deviation and Variance)

<!-- end list -->

  - Range
  - Variance: The advantage of variance is that it treats all deviations
    from the mean as the same regardless of their direction
  - Standard Deviation: -\> square root

<!-- end list -->

3.  **Correlation Coefficient**

A correlation coefficient is an indicator of how strong the relationship
between two variables is. A coefficient near +1 indicates a strong
positive correlation, a coefficient of 0 indicates no correlation, and a
coefficient near -1 indicates a strong negative correlation.

  - Correlation ≠ Causation

#### Linear Regression

  - There is a linear relationship between the dependent variable (Y)
    and the independent variable (X).

  - Homoscedasticity: a condition in which the variance of the
    residuals, or error term, in a regression model is *constant*. That
    is, the error term does not vary much as the value of the predictor
    variable changes

  - The independent variables aren’t highly correlated with each other
    (multicollinearity); aka they are not a linear transformation of the
    other.

  - The residuals follow a normal distribution and are independent of
    each other.
    (iid)

##### Error metrics:

    - MSE: Average squared difference between true and predicted values across all data points. 
    
    - RMSE: Takes the square root of the MSE
    
    - MAE: The mean absolute error takes the average absolute difference between the true and predicted.

  - What is **multicollinearity**?

Multicollinearity occurs when two or more independent variables in the
dataset are highly correlated with each other. In a regression model,
multicollinearity can harm the interpretability of the model because it
will be difficult to distinguish the individual effects of each variable
on the model. -\> Inflate the R2

  - **R-squared**?

The value of R-squared tells us the amount of change in the dependent
variable (Y) that is explained by the independent variable (X). The
R-squared value can range from 0 to 1. -\> Inflate the R2; the more
covariates, the higher the R2 by construction.

  - What is **sensitivity and specificity**?
    
      - ie. “Predict the presence of a disease”, then **sensitivity** is
        the true positive rate
      - ie. % of people who don’t have the disease who received a
        negative prediction is called **specificity**

<!-- end list -->

6.  Missing data?
    
      - Deleting missing values
      - Imputing missing values with the mean/median/mode
      - Building a machine learning model to predict the missing value
        based on other values in the dataset (KNN imputation method)
      - Replacing missing values with a constant

7.  Confounding variables

A confounding variable is a factor that affects both the dependent and
independent variables, making it seem like there is a causal
relationship between them.

For example, there is a high correlation between ice cream purchases and
forest fires. The number of forest fires and ice cream sales increases
at the same time. This is because the confounding variable between them
is heat. As temperature rises, so does ice cream sales and the risk of
forest fires.

 

#### Logistic Regression

  - Basically a Classification

### A/B Testing

A/B testing is the statistical hypothesis testing for a randomized
experiment with two variables A and B.

  - **Type I and Type II error**?
    
      - A type I error: H0 is true but is rejected.
      - A type II error: H0 is false but **isn’t** rejected.

  - What is a **p-value**?

\-\> not a “probability” but a *decision rule*.

  - **Confidence interval**

A confidence interval is a probability that the true population
parameter falls between a range of two estimates.

  - **Confidence level**

The level of confidence (for example, 95% or 99%) refers to the
certainty that the true parameter lies within the confidence interval as
multiple samples are repeatedly taken.

  - **Power of the Test**

A test’s power is the probability of correctly rejecting the null
hypothesis when it is false

  - **H0 vs. H1**?

H0 is a statistical phenomenon that is used to test for possible
rejection under the assumption that result of chance would be true.

After this, you can say that the alternative hypothesis is again a
statistical phenomenon which is contrary to the Null Hypothesis.
Usually, it is considered that the observations are a result of an
effect with some chance of variation.

  - Types of tests:
    
      - Parametric -\> yes assumption.
          - Z-Test: on proportions.
          - T-test: T-test is used when the standard deviation is
            unknown and the sample size is comparatively small.
      - Non-parametric -\> no assumptions on the parameters.
          - **1-sample Wilcoxon** signed rank test. With this test, you
            also estimate the population median and compare it to a
            reference/target value. However, the test assumes your data
            comes from a symmetric distribution (like the Cauchy
            distribution or uniform distribution).
          - **Mann-Whitney test** Use this test to compare differences
            between two independent groups when dependent variables are
            either ordinal or continuous.

### Bayesian Testing

  - Bayes Theorem: It calculates the degree of belief in a certain event
    and gives a probability of the occurrence of some statistical
    problem

  
![ p(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)} {p(D)}
](https://latex.codecogs.com/png.latex?%20p%28%5Ctheta%20%7C%20D%29%20%3D%20%5Cfrac%7Bp%28D%7C%5Ctheta%29%20p%28%5Ctheta%29%7D%20%7Bp%28D%29%7D%20
" p(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)} {p(D)} ")  

  - Bayesian A/B testing can reduce required sample size **by 75%**.
      - Steps:
          - Select your distribution based on your metric of interest
            (aka flip of a coin, binomial)
          - Calculate your prior. Based on the distribution selected
            above, we next select a conjugate prior and choose
            distribution parameters that best reflect our pre-experiment
            data.
          - Run the experiment
          - Calculate three key metrics using Monte Carlo simulations.
            These metrics are percent lift, probability of being best,
            and expected loss.
          - The binomial distribution describes our data. The conjugate
            prior of the binomial is the beta distribution, both of
            which can be seen below.
  - Results:
      - P(a) of being the best ≠ p-value
      - Uplift
